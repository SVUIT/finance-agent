import os
import pandas as pd
from typing import TypedDict, Annotated, List, Any
from dotenv import load_dotenv
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, AnyMessage
from langchain_openai import ChatOpenAI
# from sentence_transformers import SentenceTransformer
from tidb_vector.integrations import TiDBVectorClient
import uuid
# from transformers import AutoTokenizer, AutoModel
from openai import OpenAI
# import torch
# import torch.nn.functional as F
import pymysql
pymysql.install_as_MySQLdb()
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
# ----- Graph State -----
class GraphState(TypedDict):
    file_path: str
    df: pd.DataFrame
    messages: Annotated[List[AnyMessage], "Conversation messages"]


# ----- LLM -----
llm = ChatOpenAI(model='gpt-4o-mini', api_key=os.getenv("OPENAI_API_KEY"))

# ----- Agent 1: CSV Reader -----
async def csv_reader(state: GraphState) -> GraphState:
    df = pd.read_csv(state["file_path"])
    # Process the DataFrame as needed
    return {**state, "df": df}

# ----- Agent 2: Categorizer -----
async def categorize(state: GraphState) -> GraphState:
    df = state["df"]
    prompt_template = """
    Based on the name, created_at, transfer_purpose, classify the category 
    (salary, food, transportation, health, education, family, apparel, etc.) of the transaction.
    Transaction details:
    name: {name}
    created_at: {created_at}
    transfer_purpose: {transfer_purpose}
    Return only the category name, nothing else.
    """

    def classify_transaction(row):
        response = llm.invoke([
            SystemMessage(content="You are a helpful assistant that classifies financial transactions."),
            HumanMessage(content=prompt_template.format(
                name=row["name"],
                created_at=row["created_at"],
                transfer_purpose=row["transfer_purpose"]
            ))
        ])
        return response.content.strip()

    df["category"] = df.apply(classify_transaction, axis=1)

    def text_to_embedding(text: str, model="text-embedding-3-small"):
        response = client.embeddings.create(
            input=[text],
            model=model
        )
        return response.data[0].embedding
    
    vector_store = TiDBVectorClient(
        # The 'embedded_documents' table will store the vector data.
        table_name='embedded_documents',
        # The connection string to the TiDB cluster.
        connection_string=os.environ.get('TIDB_DATABASE_URL'),
        # The dimension of the vector generated by the embedding model.
        vector_dimension=1536,
        # Recreate the table if it already exists.
        drop_existing_table=True,
    )
    
    documents = []
    for _, row in df.iterrows():
        documents.append({
            "id": str(uuid.uuid4()),
            "embedding": text_to_embedding(row["transfer_purpose"]),
            "text": row["transfer_purpose"],
            "metadata": {
                "name": row["name"],
                "created_at": row["created_at"],
                "amount": row["amount"],
                "transfer_purpose": row["transfer_purpose"],
                "category": row["category"]
            }
        })

    vector_store.insert(
        ids=[doc["id"] for doc in documents],
        texts=[doc["text"] for doc in documents],
        embeddings=[doc["embedding"] for doc in documents],
        metadatas=[doc["metadata"] for doc in documents],
    )
    
    return {**state, "df": df}

